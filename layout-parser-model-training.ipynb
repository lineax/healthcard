{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Layout Parser Model Training**"]},{"cell_type":"markdown","metadata":{},"source":["# Detectron2\n","\n","Detectron2 is Facebook AI Research's next generation software system that implements state-of-the-art object detection algorithms. It is a ground-up rewrite of the previous version, Detectron, and it originates from maskrcnn-benchmark"]},{"cell_type":"markdown","metadata":{},"source":["### Installation\n","* detectron2 is not pre-installed in this kaggle docker, so let's install it.\n","* we need to know CUDA and pytorch version to install correct detectron2."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T14:30:28.324827Z","iopub.status.busy":"2023-02-06T14:30:28.324101Z","iopub.status.idle":"2023-02-06T14:30:29.416647Z","shell.execute_reply":"2023-02-06T14:30:29.415748Z","shell.execute_reply.started":"2023-02-06T14:30:28.324738Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Feb  6 14:30:29 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T14:32:24.019988Z","iopub.status.busy":"2023-02-06T14:32:24.019192Z","iopub.status.idle":"2023-02-06T14:32:24.991415Z","shell.execute_reply":"2023-02-06T14:32:24.990539Z","shell.execute_reply.started":"2023-02-06T14:32:24.019949Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2020 NVIDIA Corporation\n","Built on Wed_Jul_22_19:09:09_PDT_2020\n","Cuda compilation tools, release 11.0, V11.0.221\n","Build cuda_11.0_bu.TC445_37.28845127_0\n"]}],"source":["!nvcc --version"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T14:32:25.014005Z","iopub.status.busy":"2023-02-06T14:32:25.013750Z","iopub.status.idle":"2023-02-06T14:32:29.705047Z","shell.execute_reply":"2023-02-06T14:32:29.703995Z","shell.execute_reply.started":"2023-02-06T14:32:25.013974Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1.13.1+cpu False\n"]}],"source":["import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())"]},{"cell_type":"markdown","metadata":{},"source":["* It seems CUDA=11.0 and torch==1.7.0 is used in this kaggle docker image.\n","* See installation for details. https://detectron2.readthedocs.io/en/latest/tutorials/install.html"]},{"cell_type":"markdown","metadata":{},"source":["### Install Pre-Built Detectron2"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-02-06T14:32:29.707473Z","iopub.status.busy":"2023-02-06T14:32:29.707046Z","iopub.status.idle":"2023-02-06T14:33:24.676900Z","shell.execute_reply":"2023-02-06T14:33:24.676043Z","shell.execute_reply.started":"2023-02-06T14:32:29.707436Z"},"trusted":true},"outputs":[],"source":["!pip install \"git+https://github.com/facebookresearch/detectron2.git\""]},{"cell_type":"markdown","metadata":{},"source":["# importing libraries"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n"]}],"source":["!conda install numba "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T14:33:24.680896Z","iopub.status.busy":"2023-02-06T14:33:24.680663Z","iopub.status.idle":"2023-02-06T14:33:27.344340Z","shell.execute_reply":"2023-02-06T14:33:27.343634Z","shell.execute_reply.started":"2023-02-06T14:33:24.680867Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<Logger detectron2 (DEBUG)>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import pandas as pd \n","from tqdm import tqdm\n","from tqdm import tqdm_notebook as tqdm # progress bar\n","from datetime import datetime\n","import time\n","import matplotlib.pyplot as plt\n","#from pycocotools.coco import COCO\n","import os, json, cv2, random\n","import skimage.io as io\n","import copy\n","from pathlib import Path\n","from typing import Optional\n","\n","\n","# torch\n","import torch\n","\n","# Albumenatations\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","#from pycocotools.coco import COCO\n","from sklearn.model_selection import StratifiedKFold\n","\n","# glob\n","from glob import glob\n","\n","\n","import warnings\n","warnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.\n","\n","\n","# detectron2\n","from detectron2.structures import BoxMode\n","from detectron2 import model_zoo\n","from detectron2.config import get_cfg\n","from detectron2.data import DatasetCatalog, MetadataCatalog\n","from detectron2.engine import DefaultPredictor, DefaultTrainer, launch\n","from detectron2.evaluation import COCOEvaluator\n","from detectron2.structures import BoxMode\n","from detectron2.utils.visualizer import ColorMode\n","from detectron2.utils.logger import setup_logger\n","from detectron2.utils.visualizer import Visualizer\n","\n","from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n","from detectron2.data import detection_utils as utils\n","\n","\n","from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\n","from detectron2.data import detection_utils as utils\n","import detectron2.data.transforms as T\n","from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","\n","setup_logger()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:03:41.572357Z","iopub.status.busy":"2023-02-06T15:03:41.572046Z","iopub.status.idle":"2023-02-06T15:03:41.591428Z","shell.execute_reply":"2023-02-06T15:03:41.590681Z","shell.execute_reply.started":"2023-02-06T15:03:41.572321Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["thing_classes= ['DOB', 'EDate', 'FName', 'IDate', 'Id', 'LName', 'MName']\n","thing_classes_id= {'DOB': 0, 'EDate': 1, 'FName': 2, 'IDate': 3, 'Id': 4, 'LName': 5, 'MName': 6}\n"]}],"source":["f = open(r'data-exported\\COCO\\result.json',)\n","thing_classes  = []\n","thing_classes_id = {}\n","data_annotations=[]\n","data = json.load(f)\n","#----Images----\n","data_images=data['images']\n","#---annotations-\n","for i in data['annotations']:\n","    annot_obj ={\"id\": i['id'],\"image_id\": i['image_id'],\"category_id\":i['category_id'],\n","          \"x_min\":i['bbox'][0], #left\n","          \"y_min\":i['bbox'][1], #top\n","          \"x_max\":i['bbox'][0]+i['bbox'][2], #left+width\n","          \"y_max\":i['bbox'][1]+i['bbox'][3] #top+hieght\n","         }\n","    data_annotations.append(annot_obj) \n","#---categories-\n","for i in data['categories']:\n","    thing_classes.append(i['name'])\n","    thing_classes_id[i['name']]=i['id']\n","f.close()\n","print(\"thing_classes=\",thing_classes)\n","print(\"thing_classes_id=\",thing_classes_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-24T17:40:25.266689Z","iopub.status.busy":"2021-10-24T17:40:25.266252Z","iopub.status.idle":"2021-10-24T17:40:25.273295Z","shell.execute_reply":"2021-10-24T17:40:25.271999Z","shell.execute_reply.started":"2021-10-24T17:40:25.266569Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:03:44.541919Z","iopub.status.busy":"2023-02-06T15:03:44.541377Z","iopub.status.idle":"2023-02-06T15:03:44.559034Z","shell.execute_reply":"2023-02-06T15:03:44.558053Z","shell.execute_reply.started":"2023-02-06T15:03:44.541884Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train_meta size= 21\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>file_name</th>\n","      <th>width</th>\n","      <th>height</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>77f17f42-IMG_6637.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3c355f99-IMG_6638.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>3a9e546a-IMG_6639.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0ce52bb6-IMG_6640.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>b91bed45-IMG_6642.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>2a36462b-IMG_6643.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>80b2ed21-IMG_6644.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>f9dfa2e6-IMG_6645.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>12fb4510-IMG_6646.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>48b45ec6-IMG_6647.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>7b4825cb-IMG_6648.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>4e3ada6d-IMG_6649.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>8e81570b-IMG_6651.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>c65b76ad-IMG_6652.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>a459e673-IMG_6653.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>17fd673c-IMG_6654.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>5d02b94e-IMG_6655.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>99c5fe95-IMG_6656.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>1dfd4820-IMG_6657.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>82986c1c-IMG_6658.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>2329a4c3-IMG_6659.JPG</td>\n","      <td>3024</td>\n","      <td>4032</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    image_id              file_name  width  height\n","0          0  77f17f42-IMG_6637.JPG   3024    4032\n","1          1  3c355f99-IMG_6638.JPG   3024    4032\n","2          2  3a9e546a-IMG_6639.JPG   3024    4032\n","3          3  0ce52bb6-IMG_6640.JPG   3024    4032\n","4          4  b91bed45-IMG_6642.JPG   3024    4032\n","5          5  2a36462b-IMG_6643.JPG   3024    4032\n","6          6  80b2ed21-IMG_6644.JPG   3024    4032\n","7          7  f9dfa2e6-IMG_6645.JPG   3024    4032\n","8          8  12fb4510-IMG_6646.JPG   3024    4032\n","9          9  48b45ec6-IMG_6647.JPG   3024    4032\n","10        10  7b4825cb-IMG_6648.JPG   3024    4032\n","11        11  4e3ada6d-IMG_6649.JPG   3024    4032\n","12        12  8e81570b-IMG_6651.JPG   3024    4032\n","13        13  c65b76ad-IMG_6652.JPG   3024    4032\n","14        14  a459e673-IMG_6653.JPG   3024    4032\n","15        15  17fd673c-IMG_6654.JPG   3024    4032\n","16        16  5d02b94e-IMG_6655.JPG   3024    4032\n","17        17  99c5fe95-IMG_6656.JPG   3024    4032\n","18        18  1dfd4820-IMG_6657.JPG   3024    4032\n","19        19  82986c1c-IMG_6658.JPG   3024    4032\n","20        20  2329a4c3-IMG_6659.JPG   3024    4032"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["train_meta = pd.DataFrame(data_images)\n","train_meta = train_meta[['id', 'file_name', 'width', 'height']]\n","train_meta = train_meta.rename(columns={\"id\":\"image_id\"})\n","print(\"train_meta size=\",len(train_meta))\n","train_meta.head(25)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:03:47.678813Z","iopub.status.busy":"2023-02-06T15:03:47.678541Z","iopub.status.idle":"2023-02-06T15:03:47.697153Z","shell.execute_reply":"2023-02-06T15:03:47.696167Z","shell.execute_reply.started":"2023-02-06T15:03:47.678785Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["train_df size= 145\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>image_id</th>\n","      <th>category_id</th>\n","      <th>x_min</th>\n","      <th>y_min</th>\n","      <th>x_max</th>\n","      <th>y_max</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1174.017393</td>\n","      <td>2097.193726</td>\n","      <td>1449.882421</td>\n","      <td>2152.132411</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1804.118195</td>\n","      <td>2076.787929</td>\n","      <td>2117.601181</td>\n","      <td>2139.574997</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>1454.584666</td>\n","      <td>2086.205989</td>\n","      <td>1793.146290</td>\n","      <td>2142.714350</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  image_id  category_id        x_min        y_min        x_max  \\\n","0   0         0            2  1174.017393  2097.193726  1449.882421   \n","1   1         0            5  1804.118195  2076.787929  2117.601181   \n","2   2         0            6  1454.584666  2086.205989  1793.146290   \n","\n","         y_max  \n","0  2152.132411  \n","1  2139.574997  \n","2  2142.714350  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_df = pd.DataFrame(data_annotations)\n","print(\"train_df size=\",len(train_df))\n","train_df.head(3)"]},{"cell_type":"markdown","metadata":{},"source":["# configs"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:03:54.058686Z","iopub.status.busy":"2023-02-06T15:03:54.056869Z","iopub.status.idle":"2023-02-06T15:03:54.063591Z","shell.execute_reply":"2023-02-06T15:03:54.062646Z","shell.execute_reply.started":"2023-02-06T15:03:54.058644Z"},"trusted":true},"outputs":[],"source":["imgdir=r\"data-exported\\COCO\\images\"\n","\n","debug=False\n","split_mode=\"valid20\" # all_train Or  valid20 \n","image_Width=601\n","image_Height=792\n","\n","num_folds=5\n","Selected_fold=1 #1,2,3,4,5 "]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:03:56.668025Z","iopub.status.busy":"2023-02-06T15:03:56.667702Z","iopub.status.idle":"2023-02-06T15:03:56.677022Z","shell.execute_reply":"2023-02-06T15:03:56.676252Z","shell.execute_reply.started":"2023-02-06T15:03:56.667984Z"},"trusted":true},"outputs":[],"source":["def get_PL_data_dicts(\n","    imgdir: Path,\n","    _train_df: pd.DataFrame,\n","    _train_meta: pd.DataFrame,\n","    use_cache: bool = True,\n","    target_indices: Optional[np.ndarray] = None,\n","    debug: bool = False,\n","    data_type:str=\"train\"\n","   \n","):\n","    if debug:\n","            train_meta = train_meta.iloc[:100]  # For debug...\n","    dataset_dicts = []\n","    for index, train_meta_row in tqdm(_train_meta.iterrows(), total=len(_train_meta)):\n","                    record = {}\n","                    image_id,file_name, width,height = train_meta_row.values\n","                    filename = str(f'{imgdir}/{file_name}')\n","                    record[\"file_name\"] = filename\n","                    record[\"image_id\"] = image_id\n","                    record[\"width\"] = width\n","                    record[\"height\"] = height\n","                    objs = []\n","                    for index2, row in _train_df.query(\"image_id == @image_id\").iterrows():\n","                        class_id = row[\"category_id\"]\n","                        bbox_resized = [\n","                            float(row[\"x_min\"]),\n","                            float(row[\"y_min\"]),\n","                            float(row[\"x_max\"]),\n","                            float(row[\"y_max\"]),\n","                        ]\n","                        obj = {\n","                            \"bbox\": bbox_resized,\n","                            \"bbox_mode\": BoxMode.XYXY_ABS,\n","                            \"category_id\": class_id,\n","                        }\n","                        objs.append(obj)\n","                    record[\"annotations\"] = objs\n","                    dataset_dicts.append(record)\n","                    \n","    if target_indices is not None:\n","        dataset_dicts = [dataset_dicts[i] for i in target_indices]\n","\n","    return dataset_dicts\n","\n","                                  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!jupyter nbextension enable --py widgetsnbextension"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:04:08.630966Z","iopub.status.busy":"2023-02-06T15:04:08.630696Z","iopub.status.idle":"2023-02-06T15:04:08.827957Z","shell.execute_reply":"2023-02-06T15:04:08.827222Z","shell.execute_reply.started":"2023-02-06T15:04:08.630937Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["n_dataset 21 n_train 19\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d9a565c1f474482e9d0ae53f7c9a1775","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/21 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0b68a672524495fb746a4cf815646f6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/21 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["Data_Resister_training=\"PL_data_train\";\n","Data_Resister_valid=\"PL_data_valid\";\n","\n","if split_mode == \"all_train\":\n","    DatasetCatalog.register(\n","        Data_Resister_training,\n","        lambda: get_PL_data_dicts(\n","            imgdir,\n","            train_df,\n","            train_meta,\n","            debug=debug,\n","            data_type=\"train\"\n","        ),\n","    )\n","    MetadataCatalog.get(Data_Resister_training).set(thing_classes=thing_classes)\n","    \n","    \n","    dataset_dicts_train = DatasetCatalog.get(Data_Resister_training)\n","    metadata_dicts_train = MetadataCatalog.get(Data_Resister_training)\n","    \n","    \n","elif split_mode == \"valid20\":\n","\n","    n_dataset = len(train_meta)\n","    n_train = int(n_dataset * 0.95)\n","    print(\"n_dataset\", n_dataset, \"n_train\", n_train)\n","    rs = np.random.RandomState(12)\n","    inds = rs.permutation(n_dataset)\n","    train_inds, valid_inds = inds[:n_train], inds[n_train:]\n","    DatasetCatalog.register(\n","        Data_Resister_training,\n","        lambda: get_PL_data_dicts(\n","            imgdir,\n","            train_df,\n","            train_meta,\n","            target_indices=train_inds,\n","            debug=debug,\n","            data_type=\"train\"\n","        ),\n","    )\n","    MetadataCatalog.get(Data_Resister_training).set(thing_classes=thing_classes)\n","    \n","\n","    DatasetCatalog.register(\n","        Data_Resister_valid,\n","        lambda: get_PL_data_dicts(\n","            imgdir,\n","            train_df,\n","            train_meta,\n","            target_indices=valid_inds,\n","            debug=debug,\n","            data_type=\"val\"\n","            ),\n","        )\n","    MetadataCatalog.get(Data_Resister_valid).set(thing_classes=thing_classes)\n","    \n","    dataset_dicts_train = DatasetCatalog.get(Data_Resister_training)\n","    metadata_dicts_train = MetadataCatalog.get(Data_Resister_training)\n","\n","    dataset_dicts_valid = DatasetCatalog.get(Data_Resister_valid)\n","    metadata_dicts_valid = MetadataCatalog.get(Data_Resister_valid)\n","    \n","else:\n","    raise ValueError(f\"[ERROR] Unexpected value split_mode={split_mode}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:04:36.602392Z","iopub.status.busy":"2023-02-06T15:04:36.601695Z","iopub.status.idle":"2023-02-06T15:04:36.608313Z","shell.execute_reply":"2023-02-06T15:04:36.607398Z","shell.execute_reply.started":"2023-02-06T15:04:36.602354Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["dicts training size= 19 ################  dicts valid size= 2\n"]}],"source":["print(\"dicts training size=\",len(dataset_dicts_train),\"################  dicts valid size=\",len(dataset_dicts_valid))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:04:38.743034Z","iopub.status.busy":"2023-02-06T15:04:38.742761Z","iopub.status.idle":"2023-02-06T15:04:38.749870Z","shell.execute_reply":"2023-02-06T15:04:38.749207Z","shell.execute_reply.started":"2023-02-06T15:04:38.743003Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'file_name': 'data-exported\\\\COCO\\\\images/f9dfa2e6-IMG_6645.JPG',\n"," 'image_id': 7,\n"," 'width': 3024,\n"," 'height': 4032,\n"," 'annotations': [{'bbox': [1200.28955658144,\n","    1984.9294304464079,\n","    1524.958043197731,\n","    2088.382107081718],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 2.0},\n","  {'bbox': [2287.4370647965975,\n","    1960.297840771334,\n","    2744.924477755917,\n","    2073.6031532766738],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 6.0},\n","  {'bbox': [1524.9580431977288,\n","    1975.076794576377,\n","    2272.679406314035,\n","    2068.676835341656],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 4.0},\n","  {'bbox': [1210.1279955698105,\n","    2073.603153276671,\n","    2405.4983326570627,\n","    2211.540055457088],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 0.0},\n","  {'bbox': [1111.743605686086,\n","    2374.1085473125736,\n","    1800.4343348721573,\n","    2477.5612239478855],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 0.0},\n","  {'bbox': [1146.1781421453916,\n","    2649.9823516734036,\n","    1795.5151153779739,\n","    2733.729756568655],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 3.0},\n","  {'bbox': [1795.5151153779725,\n","    2630.277079933342,\n","    2405.4983326570646,\n","    2718.9508027636098],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 1.0}]}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["dataset_dicts_train[0]"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:04:43.354024Z","iopub.status.busy":"2023-02-06T15:04:43.353469Z","iopub.status.idle":"2023-02-06T15:04:43.360022Z","shell.execute_reply":"2023-02-06T15:04:43.359370Z","shell.execute_reply.started":"2023-02-06T15:04:43.353987Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'file_name': 'data-exported\\\\COCO\\\\images/80b2ed21-IMG_6644.JPG',\n"," 'image_id': 6,\n"," 'width': 3024,\n"," 'height': 4032,\n"," 'annotations': [{'bbox': [1101.367583801008,\n","    1926.5513021818324,\n","    1390.3183907814162,\n","    1990.4813894472386],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 2.0},\n","  {'bbox': [1806.945135729912,\n","    1913.0923364417467,\n","    2112.695408232437,\n","    1977.0224237071527],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 6.0},\n","  {'bbox': [1397.0381769902629,\n","    1926.5513021818324,\n","    1813.6649219387587,\n","    1983.7519065771958],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 5.0},\n","  {'bbox': [1101.3675838010079,\n","    1987.1166480122165,\n","    2005.1788288908892,\n","    2084.694149627837],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 0.0},\n","  {'bbox': [1111.4472631142783,\n","    2215.9190655936695,\n","    1575.112511524701,\n","    2289.9433771641397],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 3.0},\n","  {'bbox': [1108.0873700098548,\n","    2414.43881025993,\n","    1561.6729391070073,\n","    2491.8278632654215],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 3.0},\n","  {'bbox': [1565.0328322114303,\n","    2404.344585954866,\n","    1995.0991495776195,\n","    2468.2746732202722],\n","   'bbox_mode': <BoxMode.XYXY_ABS: 0>,\n","   'category_id': 1.0}]}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["dataset_dicts_valid[0]"]},{"cell_type":"markdown","metadata":{},"source":["# Data Visualization"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.status.busy":"2021-10-08T06:24:24.898373Z","iopub.status.idle":"2021-10-08T06:24:24.898792Z","shell.execute_reply":"2021-10-08T06:24:24.898591Z","shell.execute_reply.started":"2021-10-08T06:24:24.898569Z"}},"source":["fig = plt.figure(figsize =(40,40))\n","ax = fig.add_subplot(1,1,1)\n","\n","d=dataset_dicts_valid[1]   \n","img = cv2.imread(d[\"file_name\"])\n","v = Visualizer(img[:, :, ::-1],\n","                metadata=metadata_dicts_valid, \n","                scale=1.5, \n","                instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",")\n","out = v.draw_dataset_dict(d)\n","ax.grid(False)\n","ax.axis('off')\n","ax.imshow(out.get_image()[:, :, ::-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:07:16.917861Z","iopub.status.busy":"2023-02-06T15:07:16.917246Z","iopub.status.idle":"2023-02-06T15:07:18.061160Z","shell.execute_reply":"2023-02-06T15:07:18.060222Z","shell.execute_reply.started":"2023-02-06T15:07:16.917825Z"},"trusted":true},"outputs":[],"source":["fig = plt.figure(figsize =(40,40)) \n","ax = fig.add_subplot(1,1,1)\n","\n","d=dataset_dicts_valid[1]\n","img = cv2.imread(d[\"file_name\"]) \n","v = Visualizer(img[:, :, ::-1], metadata=metadata_dicts_valid, scale=1.5, instance_mode=ColorMode.IMAGE_BW) # remove the colors of unsegmented pixels. This option is only available for segmentation models ) \n","out = v.draw_dataset_dict(d) \n","ax.grid(False) \n","ax.axis('off') \n","ax.imshow(out.get_image()[:, :, ::-1])"]},{"cell_type":"markdown","metadata":{},"source":["# Data Augmentation"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-02-06T15:07:57.744250Z","iopub.status.busy":"2023-02-06T15:07:57.743974Z","iopub.status.idle":"2023-02-06T15:07:57.753085Z","shell.execute_reply":"2023-02-06T15:07:57.752389Z","shell.execute_reply.started":"2023-02-06T15:07:57.744219Z"},"trusted":true},"outputs":[],"source":["def custom_mapper(dataset_dict):\n","    \n","    dataset_dict = copy.deepcopy(dataset_dict)\n","    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n","    transform_list = [T.RandomBrightness(0.8, 1.2),\n","                      T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n","                      T.RandomFlip(prob=0.5, horizontal=True, vertical=False)\n","                      ]\n","    image, transforms = T.apply_transform_gens(transform_list, image)\n","    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n","\n","    annos = [\n","        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n","        for obj in dataset_dict.pop(\"annotations\")\n","        if obj.get(\"iscrowd\", 0) == 0\n","    ]\n","    instances = utils.annotations_to_instances(annos, image.shape[:2])\n","    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n","    return dataset_dict\n","class AugTrainer(DefaultTrainer):\n","    @classmethod\n","    def build_train_loader(cls, cfg):\n","        return build_detection_train_loader(cfg, mapper=custom_mapper)"]},{"cell_type":"code","execution_count":16,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2023-02-06T15:07:58.869631Z","iopub.status.busy":"2023-02-06T15:07:58.869035Z","iopub.status.idle":"2023-02-06T15:08:08.430276Z","shell.execute_reply":"2023-02-06T15:08:08.428759Z","shell.execute_reply.started":"2023-02-06T15:07:58.869593Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[02/07 05:08:26 d2.engine.defaults]: Model:\n","GeneralizedRCNN(\n","  (backbone): FPN(\n","    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (top_block): LastLevelMaxPool()\n","    (bottom_up): ResNet(\n","      (stem): BasicStem(\n","        (conv1): Conv2d(\n","          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n","          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","        )\n","      )\n","      (res2): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res3): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res4): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (3): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (4): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","        (5): BottleneckBlock(\n","          (conv1): Conv2d(\n","            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n","          )\n","        )\n","      )\n","      (res5): Sequential(\n","        (0): BottleneckBlock(\n","          (shortcut): Conv2d(\n","            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","          (conv1): Conv2d(\n","            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (1): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","        (2): BottleneckBlock(\n","          (conv1): Conv2d(\n","            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv2): Conv2d(\n","            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n","          )\n","          (conv3): Conv2d(\n","            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n","            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (proposal_generator): RPN(\n","    (rpn_head): StandardRPNHead(\n","      (conv): Conv2d(\n","        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n","        (activation): ReLU()\n","      )\n","      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","    (anchor_generator): DefaultAnchorGenerator(\n","      (cell_anchors): BufferList()\n","    )\n","  )\n","  (roi_heads): StandardROIHeads(\n","    (box_pooler): ROIPooler(\n","      (level_poolers): ModuleList(\n","        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n","        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n","        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n","        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n","      )\n","    )\n","    (box_head): FastRCNNConvFCHead(\n","      (flatten): Flatten(start_dim=1, end_dim=-1)\n","      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc_relu1): ReLU()\n","      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n","      (fc_relu2): ReLU()\n","    )\n","    (box_predictor): FastRCNNOutputLayers(\n","      (cls_score): Linear(in_features=1024, out_features=8, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=28, bias=True)\n","    )\n","  )\n",")\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bdfa94d77b0a43ef8dfbae7b2645a85c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/21 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[02/07 05:08:26 d2.data.build]: Removed 0 images with no usable annotations. 19 images left.\n","[02/07 05:08:26 d2.data.build]: Using training sampler TrainingSampler\n","[02/07 05:08:26 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n","[02/07 05:08:26 d2.data.common]: Serializing 19 elements to byte tensors and concatenating them all ...\n","[02/07 05:08:26 d2.data.common]: Serialized dataset takes 0.01 MiB\n","[02/07 05:08:26 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from kaggle/Weights/mask_rcnn_R_50_FPN_3x/model_final.pth ...\n"]},{"name":"stderr","output_type":"stream","text":["Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (7, 1024) in the checkpoint but (8, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (7,) in the checkpoint but (8,) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (24, 1024) in the checkpoint but (28, 1024) in the model! You might want to double check if this is expected.\n","Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (24,) in the checkpoint but (28,) in the model! You might want to double check if this is expected.\n","Some model parameters or buffers are not found in the checkpoint:\n","roi_heads.box_predictor.bbox_pred.{bias, weight}\n","roi_heads.box_predictor.cls_score.{bias, weight}\n","The checkpoint state_dict contains keys that are not used by the model:\n","  roi_heads.mask_head.mask_fcn1.{bias, weight}\n","  roi_heads.mask_head.mask_fcn2.{bias, weight}\n","  roi_heads.mask_head.mask_fcn3.{bias, weight}\n","  roi_heads.mask_head.mask_fcn4.{bias, weight}\n","  roi_heads.mask_head.deconv.{bias, weight}\n","  roi_heads.mask_head.predictor.{bias, weight}\n"]},{"name":"stdout","output_type":"stream","text":["[02/07 05:08:26 d2.engine.train_loop]: Starting training from iteration 0\n","ERROR [02/07 05:08:45 d2.engine.train_loop]: Exception during training:\n","Traceback (most recent call last):\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\engine\\train_loop.py\", line 149, in train\n","    self.run_step()\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\engine\\defaults.py\", line 494, in run_step\n","    self._trainer.run_step()\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\engine\\train_loop.py\", line 274, in run_step\n","    loss_dict = self.model(data)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\modeling\\meta_arch\\rcnn.py\", line 158, in forward\n","    features = self.backbone(images.tensor)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\modeling\\backbone\\fpn.py\", line 139, in forward\n","    bottom_up_features = self.bottom_up(x)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\modeling\\backbone\\resnet.py\", line 449, in forward\n","    x = stage(x)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 204, in forward\n","    input = module(input)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\modeling\\backbone\\resnet.py\", line 201, in forward\n","    out = self.conv3(out)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1194, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"d:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\layers\\wrappers.py\", line 113, in forward\n","    x = F.conv2d(\n","RuntimeError: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3137863680 bytes.\n","[02/07 05:08:45 d2.engine.hooks]: Total training time: 0:00:18 (0:00:00 on hooks)\n","[02/07 05:08:45 d2.utils.events]:  iter: 0    lr: N/A  \n"]},{"ename":"RuntimeError","evalue":"[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3137863680 bytes.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[16], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39m#trainer = DefaultTrainer(cfg) \u001b[39;00m\n\u001b[0;32m     39\u001b[0m trainer\u001b[39m.\u001b[39mresume_or_load(resume\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> 40\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\engine\\defaults.py:484\u001b[0m, in \u001b[0;36mDefaultTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m    Run training.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \n\u001b[0;32m    481\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[39m        OrderedDict of results, if evaluation is enabled. Otherwise None.\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtrain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstart_iter, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter)\n\u001b[0;32m    485\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mTEST\u001b[39m.\u001b[39mEXPECTED_RESULTS) \u001b[39mand\u001b[39;00m comm\u001b[39m.\u001b[39mis_main_process():\n\u001b[0;32m    486\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mhasattr\u001b[39m(\n\u001b[0;32m    487\u001b[0m             \u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_last_eval_results\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    488\u001b[0m         ), \u001b[39m\"\u001b[39m\u001b[39mNo evaluation results obtained during training!\u001b[39m\u001b[39m\"\u001b[39m\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\engine\\train_loop.py:149\u001b[0m, in \u001b[0;36mTrainerBase.train\u001b[1;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_iter, max_iter):\n\u001b[0;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbefore_step()\n\u001b[1;32m--> 149\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_step()\n\u001b[0;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter_step()\n\u001b[0;32m    151\u001b[0m \u001b[39m# self.iter == max_iter can be used by `after_train` to\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[39m# tell whether the training successfully finished or failed\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[39m# due to exceptions.\u001b[39;00m\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\engine\\defaults.py:494\u001b[0m, in \u001b[0;36mDefaultTrainer.run_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer\u001b[39m.\u001b[39miter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter\n\u001b[1;32m--> 494\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_trainer\u001b[39m.\u001b[39;49mrun_step()\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\engine\\train_loop.py:274\u001b[0m, in \u001b[0;36mSimpleTrainer.run_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    269\u001b[0m data_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m start\n\u001b[0;32m    271\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39mIf you want to do something with the losses, you can wrap the model.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 274\u001b[0m loss_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(data)\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(loss_dict, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    276\u001b[0m     losses \u001b[39m=\u001b[39m loss_dict\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\modeling\\meta_arch\\rcnn.py:158\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, batched_inputs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     gt_instances \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 158\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensor)\n\u001b[0;32m    160\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproposal_generator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproposal_generator(images, features, gt_instances)\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\modeling\\backbone\\fpn.py:139\u001b[0m, in \u001b[0;36mFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m    127\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m        input (dict[str->Tensor]): mapping feature map name (e.g., \"res5\") to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39m            [\"p2\", \"p3\", ..., \"p6\"].\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m     bottom_up_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbottom_up(x)\n\u001b[0;32m    140\u001b[0m     results \u001b[39m=\u001b[39m []\n\u001b[0;32m    141\u001b[0m     prev_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlateral_convs[\u001b[39m0\u001b[39m](bottom_up_features[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]])\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\modeling\\backbone\\resnet.py:449\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    447\u001b[0m     outputs[\u001b[39m\"\u001b[39m\u001b[39mstem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m x\n\u001b[0;32m    448\u001b[0m \u001b[39mfor\u001b[39;00m name, stage \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstage_names, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstages):\n\u001b[1;32m--> 449\u001b[0m     x \u001b[39m=\u001b[39m stage(x)\n\u001b[0;32m    450\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_out_features:\n\u001b[0;32m    451\u001b[0m         outputs[name] \u001b[39m=\u001b[39m x\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\modeling\\backbone\\resnet.py:201\u001b[0m, in \u001b[0;36mBottleneckBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n\u001b[0;32m    199\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu_(out)\n\u001b[1;32m--> 201\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(out)\n\u001b[0;32m    203\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshortcut \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    204\u001b[0m     shortcut \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshortcut(x)\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\miniconda3\\envs\\cv2\\lib\\site-packages\\detectron2\\layers\\wrappers.py:113\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mnumel() \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    108\u001b[0m             \u001b[39m# https://github.com/pytorch/pytorch/issues/12013\u001b[39;00m\n\u001b[0;32m    109\u001b[0m             \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    110\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mSyncBatchNorm\n\u001b[0;32m    111\u001b[0m             ), \u001b[39m\"\u001b[39m\u001b[39mSyncBatchNorm does not support empty inputs!\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 113\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mconv2d(\n\u001b[0;32m    114\u001b[0m     x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups\n\u001b[0;32m    115\u001b[0m )\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n","\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3137863680 bytes."]}],"source":["cfg = get_cfg()\n","config_name = \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\" \n","\n","cfg.merge_from_file(model_zoo.get_config_file(config_name))\n","\n","cfg.DATASETS.TRAIN = (Data_Resister_training,)\n","\n","if split_mode == \"all_train\":\n","    cfg.DATASETS.TEST = ()\n","else:\n","    cfg.DATASETS.TEST = (Data_Resister_valid,)\n","    cfg.TEST.EVAL_PERIOD = 1000\n","\n","cfg.DATALOADER.NUM_WORKERS = 0\n","#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_name)\n","cfg.MODEL.WEIGHTS=\"kaggle/Weights/mask_rcnn_R_50_FPN_3x/model_final.pth\"\n","\n","cfg.SOLVER.IMS_PER_BATCH = 4\n","cfg.SOLVER.BASE_LR = 0.0025\n","\n","cfg.SOLVER.WARMUP_ITERS = 10\n","cfg.SOLVER.MAX_ITER = 2000 #adjust up if val mAP is still rising, adjust down if overfit\n","cfg.SOLVER.STEPS = (500, 1000) # must be less than  MAX_ITER \n","cfg.SOLVER.GAMMA = 0.05\n","cfg.MODEL.DEVICE = \"cpu\"\n","\n","\n","cfg.SOLVER.CHECKPOINT_PERIOD = 100000  # Small value=Frequent save need a lot of storage.\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 4\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 7 \n","\n","\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","\n","\n","#Training using custom trainer defined above\n","trainer = AugTrainer(cfg) \n","#trainer = DefaultTrainer(cfg) \n","trainer.resume_or_load(resume=False)\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-06T15:08:08.431240Z","iopub.status.idle":"2023-02-06T15:08:08.431717Z","shell.execute_reply":"2023-02-06T15:08:08.431526Z","shell.execute_reply.started":"2023-02-06T15:08:08.431503Z"},"trusted":true},"outputs":[],"source":["evaluator = COCOEvaluator(Data_Resister_training, cfg, False, output_dir=\"./output/\")\n","cfg.MODEL.WEIGHTS=\"./output/model_final.pth\"\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold\n","val_loader = build_detection_test_loader(cfg, Data_Resister_training)\n","inference_on_dataset(trainer.model, val_loader, evaluator)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-24T17:52:01.021679Z","iopub.status.busy":"2021-10-24T17:52:01.021355Z","iopub.status.idle":"2021-10-24T17:52:01.123567Z","shell.execute_reply":"2021-10-24T17:52:01.12241Z","shell.execute_reply.started":"2021-10-24T17:52:01.021637Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","metrics_df = pd.read_json(\"./output/metrics.json\", orient=\"records\", lines=True)\n","mdf = metrics_df.sort_values(\"iteration\")\n","mdf.head(10).T"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-24T17:52:01.126812Z","iopub.status.busy":"2021-10-24T17:52:01.126209Z","iopub.status.idle":"2021-10-24T17:52:01.457378Z","shell.execute_reply":"2021-10-24T17:52:01.456014Z","shell.execute_reply.started":"2021-10-24T17:52:01.126772Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","mdf1 = mdf[~mdf[\"total_loss\"].isna()]\n","ax.plot(mdf1[\"iteration\"], mdf1[\"total_loss\"], c=\"C0\", label=\"train\")\n","if \"validation_loss\" in mdf.columns:\n","    mdf2 = mdf[~mdf[\"validation_loss\"].isna()]\n","    ax.plot(mdf2[\"iteration\"], mdf2[\"validation_loss\"], c=\"C1\", label=\"validation\")\n","\n","# ax.set_ylim([0, 0.5])\n","ax.legend()\n","ax.set_title(\"Loss curve\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-24T17:52:01.462911Z","iopub.status.busy":"2021-10-24T17:52:01.462224Z","iopub.status.idle":"2021-10-24T17:52:02.030139Z","shell.execute_reply":"2021-10-24T17:52:02.028918Z","shell.execute_reply.started":"2021-10-24T17:52:01.46287Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots()\n","\n","mdf1 = mdf[~mdf[\"fast_rcnn/cls_accuracy\"].isna()]\n","ax.plot(mdf1[\"iteration\"], mdf1[\"fast_rcnn/cls_accuracy\"], c=\"C0\", label=\"train\")\n","# ax.set_ylim([0, 0.5])\n","ax.legend()\n","ax.set_title(\"Accuracy curve\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-24T17:52:02.032529Z","iopub.status.busy":"2021-10-24T17:52:02.032177Z","iopub.status.idle":"2021-10-24T17:52:02.265144Z","shell.execute_reply":"2021-10-24T17:52:02.263857Z","shell.execute_reply.started":"2021-10-24T17:52:02.032487Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots()\n","mdf1 = mdf[~mdf[\"loss_box_reg\"].isna()]\n","ax.plot(mdf1[\"iteration\"], mdf1[\"loss_box_reg\"], c=\"C0\", label=\"train\")\n","# ax.set_ylim([0, 0.5])\n","ax.legend()\n","ax.set_title(\"loss_box_reg\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-10-24T17:42:27.879352Z","iopub.status.busy":"2021-10-24T17:42:27.878878Z","iopub.status.idle":"2021-10-24T17:42:43.170545Z","shell.execute_reply":"2021-10-24T17:42:43.169407Z","shell.execute_reply.started":"2021-10-24T17:42:27.87932Z"},"trusted":true},"outputs":[],"source":["!pip install -U layoutparser "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-24T17:52:02.267053Z","iopub.status.busy":"2021-10-24T17:52:02.266729Z","iopub.status.idle":"2021-10-24T17:52:02.271852Z","shell.execute_reply":"2021-10-24T17:52:02.270715Z","shell.execute_reply.started":"2021-10-24T17:52:02.267016Z"},"trusted":true},"outputs":[],"source":["import layoutparser as lp"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-24T17:52:02.274982Z","iopub.status.busy":"2021-10-24T17:52:02.274169Z","iopub.status.idle":"2021-10-24T17:52:04.110673Z","shell.execute_reply":"2021-10-24T17:52:04.108602Z","shell.execute_reply.started":"2021-10-24T17:52:02.27489Z"},"trusted":true},"outputs":[],"source":["model = lp.Detectron2LayoutModel('lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config',\n","                                 './output/model_final.pth',\n","                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8],\n","                                 label_map={0:\"None\",1: \"text\", 2: \"title\", 3: \"list\", 4:\"table\", 5:\"figure\"})\n","#######################################################\n","image = cv2.imread('/kaggle/input/papers-images/train/train/PMC3576793_00004.jpg')\n","#plt.imshow(image)\n","color_map = {\n","    'text':   'red',\n","    'title':  'blue',\n","    'list':   'green',\n","    'table':  'yellow',\n","    'figure': 'pink',\n","}\n","\n","layout_predicted = model.detect(image)\n","lp.draw_box(image,\n","              [b.set(id=f'{b.type}/{b.score:.2f}') for b in layout_predicted],\n","              color_map=color_map,\n","              show_element_id=True, id_font_size=10,\n","              id_text_background_color='black',\n","              id_text_color='white')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-24T17:52:06.877202Z","iopub.status.busy":"2021-10-24T17:52:06.876312Z","iopub.status.idle":"2021-10-24T17:52:09.618425Z","shell.execute_reply":"2021-10-24T17:52:09.616958Z","shell.execute_reply.started":"2021-10-24T17:52:06.877165Z"},"trusted":true},"outputs":[],"source":["cfg.MODEL.WEIGHTS = \"./output/model_final.pth\"\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold for this model\n","#cfg.DATASETS.TEST = (\"Data_Resister_training\", )\n","predictor = DefaultPredictor(cfg)\n","\n","###############################################################\n","\n","fig, ax = plt.subplots(2, 2, figsize =(20,20))\n","indices=[ax[0][0],ax[1][0],ax[0][1],ax[1][1] ]\n","i=-1\n","# Show some qualitative results by predicting on test set images\n","NUM_TEST_SAMPLES = 4\n","samples = random.sample(dataset_dicts_train, NUM_TEST_SAMPLES)\n","for i, sample in enumerate(samples):\n","    img = cv2.imread(sample[\"file_name\"])\n","    outputs = predictor(img)\n","    visualizer = Visualizer(img, metadata=metadata_dicts_train,scale=0.5,)\n","    visualizer = visualizer.draw_instance_predictions(\n","        outputs[\"instances\"].to(\"cpu\"))\n","    display_img = visualizer.get_image()[:, :, ::-1]\n","    indices[i].grid(False)\n","    indices[i].imshow(display_img)"]},{"cell_type":"markdown","metadata":{},"source":["### References\n","* https://layout-parser.readthedocs.io/en/latest/notes/installation.html\n","* https://github.com/Layout-Parser/layout-model-training/blob/master/tools/train_net.py\n","* https://towardsdatascience.com/auto-parse-and-understand-any-document-5d72e81b0be9\n","* https://layout-parser.readthedocs.io/en/latest/api_doc/models.html"]}],"metadata":{"kernelspec":{"display_name":"cv2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"vscode":{"interpreter":{"hash":"7829d3be20250e66dedc64c9dad7bd315d3260661de1c8c9caa001aa032b5716"}}},"nbformat":4,"nbformat_minor":4}
